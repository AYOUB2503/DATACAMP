# -*- coding: utf-8 -*-
"""Wind-Power

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/173cpYtamqvcPyVgfdoK5bwIzK7NmQ_CX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
import sklearn.datasets
from sklearn.model_selection import train_test_split
from sklearn import ensemble
from sklearn import datasets
from sklearn.datasets import make_regression
from sklearn.linear_model import  Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score,mean_squared_error
from sklearn.linear_model import SGDRegressor

train_x=pd.read_csv("/content/X_train_v2.csv")
train_y=pd.read_csv("/content/Y_train_sl9m6Jh.csv")
test_x=pd.read_csv("/content/X_test_v2.csv")

train_x

train_y

train_y.info()
#y'en a pas de valeurs manquantes

"""# Data visualization

## Missing data
"""

#number of missing data for each column in the training set 
print(len(train_x) - train_x.count()),len(train_x)

#train_x["NWP1_18h_D-2_U"].isnull().sum()
#missing data 

def missing_percentage(data):
    
    '''A function for showing missing data values'''
    
    total = data.isnull().sum().sort_values(ascending=False)[data.isnull().sum().sort_values(ascending=False) != 0]
    percent = (data.isnull().sum().sort_values(ascending=False) / len(data) *100)[(data.isnull().sum().sort_values(ascending=False) / len(data) * 100) != 0]
    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])


missing = missing_percentage(train_x)

fig, ax = plt.subplots(figsize=(20, 5))
sns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')
plt.xticks(rotation=90)
plt.grid(axis="y")
display(missing.T.style.background_gradient(cmap='Reds', axis=1))

#Pot de la matrice de corrélation 
df=pd.concat([train_x,train_y.Production],axis=1)
corrMatrix = df.corr()
f, ax = plt.subplots(figsize=(100, 100))
sns.heatmap(corrMatrix, annot=True,)
plt.show()

#Matrice de corrélation 
df.corr()

missing.head(12)

train_x=train_x.drop(["NWP4_00h_D-2_CLCT","NWP4_00h_D-2_U","NWP4_00h_D-2_V","NWP3_18h_D_U","NWP3_18h_D_V","NWP3_18h_D_T","NWP2_12h_D_U","NWP2_12h_D_V","NWP3_12h_D_T","NWP3_12h_D_U"],axis = 1)

train_x

# remplir les na par le median 
#for column in train_x.columns:
 # train_x[str(column)] = train_x[str(column)].fillna(train_x[str(column)].median())
#time=train_x.Time
#wf=train_x.WF
#train_x[train_x.columns] = train_x[train_x.columns].apply(pd.to_numeric, errors='coerce')
#train_x = train_x.fillna(train_x.median())
#train_x.WF=wf
#train_x.Time=time
x_train=train_x[:30000].drop(["ID","Time","WF"],axis=1)
x_test=train_x[30000:].drop(["ID","Time","WF"],axis=1)
y_train=train_y[:30000].drop(["ID"],axis=1)
y_test=train_y[30000:].drop(["ID"],axis=1)

from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
st=StandardScaler()
pipe = make_pipeline(SimpleImputer(strategy="median"),st, Ridge(alpha=10))
#SGDRegressor(loss="squared_loss",penalty="l2",alpha=1,max_iter=20000,random_state=42)

pipe.fit(x_train,y_train)

from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score,r2_score

mean_squared_error(y_test,pipe.predict(x_test),squared=False),r2_score(y_train,pipe.predict(x_train))

y_test.std()

sns.distplot(y_train.Production)

#Random forest
pipe = make_pipeline(SimpleImputer(),st, RandomForestRegressor(n_estimators=100,criterion="mse",bootstrap=True))

# Commented out IPython magic to ensure Python compatibility.
import time 
# %time
pipe.fit(x_train,y_train)

pipe.score(x_train,y_train),pipe.score(x_test,y_test)

#test_x=test_x.drop(["NWP4_00h_D-2_CLCT","NWP4_00h_D-2_U","NWP4_00h_D-2_V","NWP3_18h_D_U","NWP3_18h_D_V","NWP3_18h_D_T","NWP2_12h_D_U","NWP2_12h_D_V","NWP3_12h_D_T","NWP3_12h_D_U"],axis = 1)

[mean_squared_error(y_test,pipe.predict(x_test)),mean_squared_error(y_train,pipe.predict(x_train),squared=False),pipe.score(x_train,y_train),pipe.score(x_test,y_test)]

plt.plot(train_y.Production)
plt.show()

# remplir les na par le median 

time=train_x.Time
wf=train_x.WF
train_x[train_x.columns] = train_x[train_x.columns].apply(pd.to_numeric, errors='coerce')
train_x = train_x.fillna(train_x.median())
train_x.drop(["WF","Time","ID"],axis=1)

x_train=train_x[:30000].drop(["ID","Time","WF"],axis=1)
x_test=train_x[30000:].drop(["ID","Time","WF"],axis=1)
y_train=train_y[:30000].drop(["ID"],axis=1)
y_test=train_y[30000:].drop(["ID"],axis=1)

# Hyperparameter tuning 
from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [100,200]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [5,7,10]
max_depth.append(None)
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

from tqdm import tqdm
# First create the base model to tune
rf = RandomForestRegressor(verbose=True)

rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid)
# Fit the random search model
rf_random.fit(x_train, y_train.Production)

rf_random.best_params_

[mean_squared_error(y_test,rf_random.predict(x_test)),mean_squared_error(y_train,rf_random.predict(x_train),squared=False),rf_random.score(x_train,y_train),rf_random.score(x_test,y_test)]

